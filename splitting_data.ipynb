{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a file for the splitting of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of samples: 113\nThe number of features: 160\nIteration 1\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 2\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 3\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 4\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 5\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 6\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 7\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 8\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 9\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 10\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 11\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 12\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 13\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 14\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 15\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 16\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 17\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 18\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 19\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\nIteration 20\nTraining dataset size 90\nTest dataset size 23\nValidation size in current fold = 23\nValidation size in current fold = 23\nValidation size in current fold = 22\nValidation size in current fold = 22\n"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from hn.load_data import load_data\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of features: {len(data.columns)}')\n",
    "Y = data['label']\n",
    "\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=0)\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4)\n",
    "\n",
    "count = 0 \n",
    "for train_index, test_index in sss.split(data, Y):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    count += 1\n",
    "    print(f'Iteration {count}')\n",
    "    data_array = pd.DataFrame.from_dict(data) #Needed to be able to index this later \n",
    "    split_X_train = data_array.iloc[train_index]\n",
    "    split_y_train = Y[train_index]\n",
    "    split_X_test = data_array.iloc[test_index]\n",
    "    split_y_test = Y[test_index]\n",
    "    print(f'Training dataset size {len(split_X_train)}')\n",
    "    print(f'Test dataset size {len(split_X_test)}')\n",
    "\n",
    "    # Loop over the folds\n",
    "    for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "        X_validation = split_X_train.iloc[validation_index]\n",
    "        y_validation = split_y_train[validation_index]\n",
    "        X_train = split_X_train.iloc[training_index]\n",
    "        y_train = split_y_train[training_index]\n",
    "        print(f'Validation size in current fold = {len(X_validation)}')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'T34'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-85900b91510b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# scaler = preprocessing.MinMaxScaler()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# scaler = preprocessing.RobustScaler()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    698\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'),\n\u001b[1;32m    699\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'T34'"
     ]
    }
   ],
   "source": [
    "# General packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "import seaborn\n",
    "\n",
    "# Classifiers\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Data preprocessing\n",
    "# Input data is X_train en y_train\n",
    "\n",
    "# 1. Scaling (standard, min max, robust)\n",
    "\n",
    "# Scale the data to be normal\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# 2. Feature selection/extraction\n",
    "# Create the Recursive Feature Elimination object and compute a cross-validated score.\n",
    "svc = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "# classifications\n",
    "rfecv = feature_selection.RFECV(\n",
    "    estimator=svc, step=1, \n",
    "    cv=model_selection.StratifiedKFold(4),\n",
    "    scoring='roc_auc')\n",
    "rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# 3. PCA\n",
    "n_selected_features = rfecv.n_features_\n",
    "n_samples = 113  # moeten we nog uit de data halen, is netter\n",
    "n_components == min(n_samples, n_selected_features)\n",
    "pca = decomposition.PCA(n_components)\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "seaborn.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}