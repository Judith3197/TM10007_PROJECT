{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda3e433ad4382c432eac770014b82f80bd",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment TM10007 - Machine Learning\n",
    "Group 3\n",
    "Noor Borren\n",
    "Puck Groen 4470044\n",
    "Lucy Kn√∂ps \n",
    "Judith Sluijter"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from hn.load_data import load_data\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn import model_selection, metrics, feature_selection, preprocessing, neighbors, decomposition, svm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def data_preprocessing_pca(X_train, X_validation):\n",
    "    ''' Data preprocessing: first scaling and then PCA with a optimized number of components '''\n",
    "\n",
    "    # 1. Scaling \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "    # 2. Principle component analysis\n",
    "    # Using the cumulative summation of the explained variance, we concluded that in\n",
    "    # order to retain 95% of the variance 30 components are needed.\n",
    "\n",
    "    pca = PCA(n_components=30)\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_validation_pca = pca.transform(X_validation_scaled)\n",
    "\n",
    "    return X_train_pca, X_validation_pca\n",
    "\n",
    "\n",
    "def data_preprocessing_uni(X_train, y_train, X_validation):\n",
    "    '''Data preprocessing: first scaling and then univariate analysis'''\n",
    "\n",
    "    # 1. Scaling \n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    df_X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "    X_validation_scaled = scaler.transform(X_validation)\n",
    "    df_X_validation_scaled = pd.DataFrame(X_validation_scaled)\n",
    "\n",
    "    bestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "    fit = bestfeatures.fit(df_X_train_scaled, y_train)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(df_X_train_scaled.columns)\n",
    "    # Concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  # Naming the dataframe columns\n",
    "    best_features = featureScores.nlargest(10,'Score')['Specs']\n",
    "\n",
    "    X_train_uni = df_X_train_scaled[best_features]\n",
    "    X_validation_uni = df_X_validation_scaled[best_features]\n",
    "\n",
    "    return X_train_uni, X_validation_uni\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores  = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of the data \n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of features: {len(data.columns)-1}')\n",
    "y_labels = data['label']\n",
    "del data['label']\n",
    "\n",
    "y = preprocessing.label_binarize(y_labels, ['T12', 'T34']) \n",
    "# 0 stands for T12 and 1 for T34\n",
    "y = [i[0] for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data in a train and test set\n",
    "split_X_train, split_X_test, split_y_train, split_y_test = train_test_split(data, y, stratify=y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the data\n",
    "For the preprocessing of the data, we always started with scaling the data. After the scaling we tried different preprocessing steps. First, we tried the Feature ranking with recursive feature elimination, this is shown in appendix 1.a. This didn't work as well as we wanted to, so we tried two different preprocessing steps: the Principal Component Analysis (PCA) and the Univariate analysis. These two options for datapreprocessing are shown in the cell with functions (data_preprocessing_pca, data_preprocessing_uni). We applied both of the preprocessing functions to different classifiers and compared the results. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "After the preprocessing we applied different classifiers to see which one gave the best result. We applied the k-Nearest Neighbor classifier (kNN), the Logistic Regression classifier, the Random Forest classifier and the Support Vector Machine classifier. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Classifier\n",
    "We tried the kNN classifier with the two different preprocessing methods. First with PCA and second with Univariate Analysis, the code is provided below."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: kNN\n",
    "# Preprocessing: PCA\n",
    "# Dataset: training & validation\n",
    "\n",
    "cv_4fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "k_list = list(range(1, 50, 2))\n",
    "all_train = []\n",
    "all_validation = []\n",
    "for _ in range(0,20):\n",
    "    for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "        train_scores = []\n",
    "        validation_scores = []\n",
    "        X_validation = split_X_train.iloc[validation_index]\n",
    "        y_validation = split_y_train[validation_index]\n",
    "        X_train = split_X_train.iloc[training_index]\n",
    "        y_train = split_y_train[training_index]\n",
    "\n",
    "        # Preprocessing using PCA\n",
    "        X_train_pca, X_validation_pca = data_preprocessing_pca(X_train, X_validation)\n",
    "\n",
    "        for k in k_list:\n",
    "            clf_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            clf_knn.fit(X_train_pca, y_train)\n",
    "\n",
    "            # Test the classifier on the training data and plot\n",
    "            train_proba = clf_knn.predict_proba(X_train_pca)[:, 1]\n",
    "            validation_proba = clf_knn.predict_proba(X_validation_pca)[:, 1]\n",
    "            \n",
    "            score_train = roc_auc_score(y_train, train_proba)\n",
    "            score_validation = roc_auc_score(y_validation, validation_proba)\n",
    "\n",
    "            train_scores.append(score_train)\n",
    "            validation_scores.append(score_validation)\n",
    "\n",
    "        all_train.append(train_scores)\n",
    "        all_validation.append(validation_scores)\n",
    "\n",
    "# Create numpy array of scores and calculate the mean and std\n",
    "all_train = np.array(all_train)\n",
    "all_validation = np.array(all_validation)\n",
    "\n",
    "train_scores_mean = all_train.mean(axis=0)\n",
    "train_scores_std = all_train.std(axis=0)\n",
    "\n",
    "validation_scores_mean = all_validation.mean(axis=0)\n",
    "validation_scores_std = all_validation.std(axis=0)\n",
    "\n",
    "# Plot the learning curve (mean scores and std as shading)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid()\n",
    "ax.fill_between(k_list, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "ax.fill_between(k_list, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "ax.plot(k_list, train_scores_mean, 'o-', color=\"r\",\n",
    "        label=\"Training score\")\n",
    "ax.plot(k_list, validation_scores_mean, 'o-', color=\"g\",\n",
    "        label=\"Validation score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: kNN\n",
    "# Preprocessing: PCA\n",
    "# Dataset: training & validation\n",
    "\n",
    "# kNN will be plotted for the scoring of the classifiers\n",
    "\n",
    "k_list = list(range(1, 50, 2))\n",
    "all_train = []\n",
    "all_validation = []\n",
    "for _ in range(0,20):\n",
    "    for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "        train_scores = []\n",
    "        validation_scores = []\n",
    "        X_validation = split_X_train.iloc[validation_index]\n",
    "        y_validation = split_y_train[validation_index]\n",
    "        X_train = split_X_train.iloc[training_index]\n",
    "        y_train = split_y_train[training_index]\n",
    "\n",
    "        # Preprocessing using PCA\n",
    "        X_train_pca, X_validation_pca = data_preprocessing_pca(X_train, X_validation)\n",
    "\n",
    "        for k in k_list:\n",
    "            clf_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            clf_knn.fit(X_train_pca, y_train)\n",
    "\n",
    "            # Test the classifier on the training data and plot\n",
    "            score_train = clf_knn.score(X_train_pca, y_train)\n",
    "            score_validation = clf_knn.score(X_validation_pca, y_validation)            \n",
    "\n",
    "            train_scores.append(score_train)\n",
    "            validation_scores.append(score_validation)\n",
    "\n",
    "        all_train.append(train_scores)\n",
    "        all_validation.append(validation_scores)\n",
    "\n",
    "# Create numpy array of scores and calculate the mean and std\n",
    "all_train = np.array(all_train)\n",
    "all_validation = np.array(all_validation)\n",
    "\n",
    "train_scores_mean = all_train.mean(axis=0)\n",
    "train_scores_std = all_train.std(axis=0)\n",
    "\n",
    "validation_scores_mean = all_validation.mean(axis=0)\n",
    "validation_scores_std = all_validation.std(axis=0)\n",
    "\n",
    "# Plot the learning curves (mean scores and the std as shading)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid()\n",
    "ax.fill_between(k_list, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "ax.fill_between(k_list, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "ax.plot(k_list, train_scores_mean, 'o-', color=\"r\",\n",
    "        label=\"Training score\")\n",
    "ax.plot(k_list, validation_scores_mean, 'o-', color=\"g\",\n",
    "        label=\"Validation score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graphics show the trend of the kNN. Another method for obtaining the exact number of neighbors which works best in a certain fold is using a GridSearch. The use of a GridSearch is shown below. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: kNN\n",
    "# Preprocessing: PCA\n",
    "# Dataset: training & validation\n",
    "# Use of GridSearchCV to determine k \n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Specify the search range, this could be multiple parameters for more complex classifiers\n",
    "parameters = {\"n_neighbors\": list(range(1, 50, 2))}\n",
    "\n",
    "cv_5fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "grid_search = GridSearchCV(clf_knn, parameters, cv=cv_5fold, scoring='roc_auc')\n",
    "\n",
    "result = []\n",
    "for _ in range(20):\n",
    "    X_train_pca, X_validation_pca = data_preprocessing_pca(split_X_train, X_validation)\n",
    "    grid_search.fit(X_train_pca, split_y_train)\n",
    "    print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the results above, the optimal number of neighbors varies a lot between folds. Below we applied the Gridsearch to the method and applied it to the test set."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: kNN\n",
    "# Preprocessing: PCA\n",
    "# Dataset: test\n",
    "# Use of GridSearchCV to determine k\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Specify the search range, this could be multiple parameters for more complex classifiers\n",
    "parameters = {\"n_neighbors\": list(range(1, 50, 2))}\n",
    "\n",
    "cv_4fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "grid_search = GridSearchCV(clf_knn, parameters, cv=cv_4fold, scoring='roc_auc')\n",
    "\n",
    "train_all = []\n",
    "test_all = []\n",
    "\n",
    "for _ in range(10):\n",
    "    for train_index, test_index in cv_4fold.split(data, y):\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        X_test = data.iloc[test_index]\n",
    "        y_test = y[test_index]\n",
    "        X_train = data.iloc[train_index]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        # Data preprocessing\n",
    "        X_train_pca, X_test_pca = data_preprocessing_pca(X_train, X_test)\n",
    "        grid_search.fit(X_train_pca, y_train) \n",
    "        clf_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Test\n",
    "        train_proba = clf_knn.predict_proba(X_train_pca)[:, 1]\n",
    "        test_proba = clf_knn.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "        score_train = roc_auc_score(y_train, train_proba)\n",
    "        score_test = roc_auc_score(y_test, test_proba)\n",
    "\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "\n",
    "    train_all.append(train_scores)\n",
    "    test_all.append(test_scores)\n",
    "\n",
    "# Create numpy array of scores and calculate the mean and std\n",
    "all_train = np.array(train_all)\n",
    "all_test = np.array(test_all)\n",
    "\n",
    "train_scores_mean = all_train.mean(axis=0)\n",
    "train_scores_std = all_train.std(axis=0)\n",
    "\n",
    "test_scores_mean = all_test.mean(axis=0)\n",
    "test_scores_std = all_test.std(axis=0)\n",
    "print(f'AUC: {test_scores_mean} +- {test_scores_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are quite good if we want to achieve an AUC of 70% or higher. However, the AUC varies and there is also a big variance in the standard deviation. Therefore, we want to adapt our model to achieve better and more robust results. We will now look at the Univariate Analysis as a preprocessing step. Below, only the train-validation curve is plotted."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: kNN\n",
    "# Preprocessing: Univariate Analysis\n",
    "# Dataset: training & validation\n",
    "\n",
    "# AUROC of different values for K in kNN\n",
    "\n",
    "cv_4fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "k_list = list(range(1, 50, 2))\n",
    "all_train = []\n",
    "all_test = []\n",
    "for _ in range(0,20):\n",
    "    for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        X_validation = split_X_train.iloc[validation_index]\n",
    "        y_validation = split_y_train[validation_index]\n",
    "        X_train = split_X_train.iloc[training_index]\n",
    "        y_train = split_y_train[training_index]\n",
    "\n",
    "        # Preprocessing using PCA\n",
    "        X_train_uni, X_validation_uni = data_preprocessing_uni(X_train, y_train, X_validation)\n",
    "\n",
    "        for k in k_list:\n",
    "            clf_knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "            clf_knn.fit(X_train_uni, y_train)\n",
    "\n",
    "            # Test the classifier on the training data and plot\n",
    "            train_proba = clf_knn.predict_proba(X_train_uni)[:, 1]\n",
    "            test_proba = clf_knn.predict_proba(X_validation_uni)[:, 1]\n",
    "            \n",
    "            score_train = roc_auc_score(y_train, train_proba)\n",
    "            score_test = roc_auc_score(y_validation, test_proba)\n",
    "\n",
    "            train_scores.append(score_train)\n",
    "            test_scores.append(score_test)\n",
    "\n",
    "        all_train.append(train_scores)\n",
    "        all_test.append(test_scores)\n",
    "\n",
    "# Create numpy array of scores and calculate the mean and std\n",
    "all_train = np.array(all_train)\n",
    "all_test = np.array(all_test)\n",
    "\n",
    "train_scores_mean = all_train.mean(axis=0)\n",
    "train_scores_std = all_train.std(axis=0)\n",
    "\n",
    "test_scores_mean = all_test.mean(axis=0)\n",
    "test_scores_std = all_test.std(axis=0)\n",
    "\n",
    "# Plot the learning curves (mean scores and the std as shading)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid()\n",
    "ax.fill_between(k_list, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "ax.fill_between(k_list, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "ax.plot(k_list, train_scores_mean, 'o-', color=\"r\",\n",
    "        label=\"Training score\")\n",
    "ax.plot(k_list, test_scores_mean, 'o-', color=\"g\",\n",
    "        label=\"Test score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows a higher AUROC with an AUC score above the 0.7. Next, the hyperparameter tuning using GridSearch is applied in the 'inner' loop on the test set. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: kNN\n",
    "# Preprocessing: Univariate Analysis\n",
    "# Dataset: test\n",
    "# Use of GridSearchCV to determine k\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\"n_neighbors\": list(range(1, 50, 2))}\n",
    "\n",
    "cv_4fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "grid_search = GridSearchCV(clf_knn, parameters, cv=cv_4fold, scoring='roc_auc')\n",
    "\n",
    "train_all = []\n",
    "test_all = []\n",
    "for _ in range(4):\n",
    "    for train_index, test_index in cv_4fold.split(data, y):\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        X_test = data.iloc[test_index]\n",
    "        y_test = y[test_index]\n",
    "        X_train = data.iloc[train_index]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        # Data preprocessing\n",
    "        X_train_uni, X_test_uni = data_preprocessing_uni(X_train, y_train, X_test)\n",
    "\n",
    "        grid_search.fit(X_train_uni, y_train) \n",
    "        clf_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Test\n",
    "        train_proba = clf_knn.predict_proba(X_train_uni)[:, 1]\n",
    "        test_proba = clf_knn.predict_proba(X_test_uni)[:, 1]\n",
    "\n",
    "        score_train = roc_auc_score(y_train, train_proba)\n",
    "        score_test = roc_auc_score(y_test, test_proba)\n",
    "\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "\n",
    "    train_all.append(train_scores)\n",
    "    test_all.append(test_scores)\n",
    "\n",
    "# Create numpy array of scores and calculate the mean and std\n",
    "all_train = np.array(train_all)\n",
    "all_test = np.array(test_all)\n",
    "\n",
    "train_scores_mean = all_train.mean(axis=0)\n",
    "train_scores_std = all_train.std(axis=0)\n",
    "\n",
    "test_scores_mean = all_test.mean(axis=0)\n",
    "test_scores_std = all_test.std(axis=0)\n",
    "print(f'AUC: {test_scores_mean} +- {test_scores_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results using Univariate Analysis as a preprocessing step are better than using PCA."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: Logistic Regression\n",
    "# Preprocessing: PCA\n",
    "# Dataset: train & validation \n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "# Set up the GridSearch with a set of parameters\n",
    "cv_4fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "grid_param = {'penalty' : ['l1', 'l2'], 'C' : np.logspace(-4, 4, 20),'solver' : ['liblinear']}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid=grid_param, cv=cv_4fold,n_jobs=-1,scoring='roc_auc') \n",
    "\n",
    "X_train_pca, _ = data_preprocessing_pca(split_X_train, split_X_test)\n",
    "grid_search.fit(X_train_pca, split_y_train)\n",
    "\n",
    "# Show the complete results of the cross validation\n",
    "pd.DataFrame(grid_search.cv_results_)\n",
    "print(f'Best GridSearchCV of Logistic Regression within the 4-fold cross-validation for PCA preprocessing: {grid_search.best_score_}')\n",
    "\n",
    "title = 'Learning curve for Logistic Regression classifier'\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_learning_curve(grid_search.best_estimator_, title, X_train_pca, split_y_train, ax, ylim=(0.3, 1.01), cv=cv_4fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using PCA in the preprocessing method, a lot of variance can still be seen from the learning curve, both in the graph as when we run it multiple times. \n",
    "\n",
    "To generalize better, we tried a Univariate feature selection below."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: Logistic Regression\n",
    "# Preprocessing: Univariate Analysis\n",
    "# Dataset: train & validation \n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "# Set up the GridSearch with a set of parameters\n",
    "cv_4fold = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "grid_param = {'penalty' : ['l1', 'l2'],'C' : np.logspace(-4, 4, 20),'solver' : ['liblinear']}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid=grid_param, cv=cv_4fold,n_jobs=-1,scoring='roc_auc') \n",
    "\n",
    "# Data preprocessing \n",
    "X_train_uni, _ = data_preprocessing_uni(split_X_train, split_y_train, split_X_test)\n",
    "grid_search.fit(X_train_uni, split_y_train)\n",
    "\n",
    "# Show the complete results of the cross validation\n",
    "pd.DataFrame(grid_search.cv_results_)\n",
    "print(f'Best GridSearchCV of Logistic Regression within the 4-fold cross-validation for Univariate preprocessing: {grid_search.best_score_}')\n",
    "\n",
    "title = 'Learning curve for Logistic Regression classifier'\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_learning_curve(grid_search.best_estimator_, title, X_train_uni, split_y_train, ax, ylim=(0.3, 1.01), cv=cv_4fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of Univariate Analysis seems to improve the AUC score. This can both be seen in the attribute of GridSearchCV.best_score_ as can be seen in the learning curve graph where the training and validation error seem to converge within the sample size that we have. Next, we apply the method on the test data."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: Logistic Regression\n",
    "# Preprocessing: Univariate Analysis\n",
    "# Dataset: test\n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4, shuffle=True)\n",
    "grid_param = {'penalty' : ['l1', 'l2'],'C' : np.logspace(-4, 4, 20),'solver' : ['liblinear']}\n",
    "grid_search = GridSearchCV(LogisticRegression(),param_grid=grid_param, cv=cv_4fold,n_jobs=-1, scoring='roc_auc') \n",
    "\n",
    "train_all = []\n",
    "test_all = []\n",
    "for _ in range(4):\n",
    "    for train_index, test_index in cv_4fold.split(data, y):\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        X_test = data.iloc[test_index]\n",
    "        y_test = y[test_index]\n",
    "        X_train = data.iloc[train_index]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        # Data preprocessing\n",
    "        X_train_uni, X_validation_uni = data_preprocessing_uni(X_train, y_train, X_test)\n",
    "\n",
    "        grid_search.fit(X_train_uni, y_train) \n",
    "        clf_lr = grid_search.best_estimator_\n",
    "        \n",
    "        # Test\n",
    "        train_proba = clf_lr.predict_proba(X_train_uni)[:, 1]\n",
    "        test_proba = clf_lr.predict_proba(X_validation_uni)[:, 1]\n",
    "\n",
    "        score_train = metrics.roc_auc_score(y_train, train_proba)\n",
    "        score_test = metrics.roc_auc_score(y_test, test_proba)\n",
    "\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "\n",
    "    train_all.append(train_scores)\n",
    "    test_all.append(test_scores)\n",
    "\n",
    "# Create numpy array of scores and calculate the mean and std\n",
    "all_train = np.array(train_all)\n",
    "all_test = np.array(test_all)\n",
    "\n",
    "train_scores_mean = all_train.mean(axis=0)\n",
    "train_scores_std = all_train.std(axis=0)\n",
    "\n",
    "test_scores_mean = all_test.mean(axis=0)\n",
    "test_scores_std = all_test.std(axis=0)\n",
    "print(f'The resulting method gives an AUC of {test_scores_mean} +- {test_scores_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "1. Preprocessing\n",
    "2. Classifiers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X_train, y_train):\n",
    "    '''Data preprocessing: Scaling, RFECV, PCA, Imputation missing data'''\n",
    "\n",
    "    # 1. Scaling\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "    # scaler = preprocessing.RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "    # 2. Feature selection/extraction\n",
    "    # Create the Recursive Feature Elimination object and compute a cross-validated score.\n",
    "    svc = svm.SVC(kernel=\"linear\")\n",
    "\n",
    "    rfecv = feature_selection.RFECV(estimator=svc, step=1, cv=model_selection.StratifiedKFold(4), scoring='roc_auc')\n",
    "    rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "    # 3. PCA\n",
    "    n_selected_features = rfecv.n_features_\n",
    "    n_samples = X_train.shape\n",
    "    n_components == min(n_samples, n_selected_features)\n",
    "    pca = decomposition.PCA(n_components)\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "    # 4. Imputation missing data\n",
    "    missing_values=[0.0, 1.0]\n",
    "    for elem in missing_values:\n",
    "        imputer = KNNImputer(missing_values=elem, n_neighbors=5, weights='uniform')\n",
    "        X_train_imputed = imputer.fit_transform(X_train_pca)\n",
    "\n",
    "    return X_train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b PCA\n",
    "To determine the number of components for PCA we used the Cumulative Summation of the Explained Variance. To retain 95% of the variance, 30 components are needed."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining number of components for PCA\n",
    "\n",
    "# Loading of the data \n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of features: {len(data.columns)-1}')\n",
    "y_labels = data['label']\n",
    "del data['label']\n",
    "\n",
    "y = sklearn.preprocessing.label_binarize(y_labels, ['T12', 'T34']) \n",
    "# 0 now stands for T12 and 1 for T34\n",
    "y = [i[0] for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4, shuffle=True)\n",
    "split_X_train, split_X_test, split_y_train, split_y_test = train_test_split(data, y, stratify=y,test_size=0.2)\n",
    "\n",
    "# Loop over the folds\n",
    "for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "    X_validation = split_X_train.iloc[validation_index]\n",
    "    y_validation = split_y_train[validation_index]\n",
    "    X_train = split_X_train.iloc[training_index]\n",
    "    y_train = split_y_train[training_index]\n",
    "\n",
    "    # Scale data\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "    #PCA\n",
    "    pca = PCA().fit(X_train_scaled)\n",
    "\n",
    "# Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Random Forest Clasifier\n",
    "We also tried the Random Forest Classifier. \n",
    "Summary of this classifier: The code below shows the set-up for the random forest classifier. First, the random forest classifier with a PCA preprocessing is applied. Despite tuning of the hyper parameters of the random forest, a lot of variance was present in the accuracy. Then, we performed the random forest classifier with the Univariate Analysis as preprocessing step. This resulted in a slightly higher mean accuracy, but still with a large variance. We concluded that the random forest classifier is not the optimal choice for our dataset. The model is to complex for our dataset."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing function for Random Forest classifier\n",
    "\n",
    "\n",
    "def data_preprocessing_uni_rf(X_train, y_train, X_validation):\n",
    "    '''Data preprocessing: first scaling and then univariate analysis'''\n",
    "\n",
    "    # 1. Scaling \n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    df_X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "    X_validation_scaled = scaler.transform(X_validation)\n",
    "    df_X_validation_scaled = pd.DataFrame(X_validation_scaled)\n",
    "\n",
    "    bestfeatures = SelectKBest(mutual_info_classif, k=20)\n",
    "    fit = bestfeatures.fit(df_X_train_scaled, y_train)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(df_X_train_scaled.columns)\n",
    "    # Concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  # Naming the dataframe columns\n",
    "    best_features = featureScores.nlargest(20,'Score')['Specs']\n",
    "\n",
    "    X_train_uni = df_X_train_scaled[best_features]\n",
    "    X_validation_uni = df_X_validation_scaled[best_features]\n",
    "\n",
    "    return X_train_uni, X_validation_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: Random Forest\n",
    "# Preprocessing: PCA\n",
    "# Dataset: train & validation\n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "# Loading of the data \n",
    "data = load_data()\n",
    "y_labels = data['label']\n",
    "del data['label']\n",
    "\n",
    "y = preprocessing.label_binarize(y_labels, ['T12', 'T34']) \n",
    "# 0 stands for T12 and 1 for T34\n",
    "y = [i[0] for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data in a train and test set\n",
    "split_X_train, split_X_test, split_y_train, split_y_test = train_test_split(data, y, stratify=y, test_size=0.2)\n",
    "\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "# Loop over the folds\n",
    "num = 0\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "    X_validation = split_X_train.iloc[validation_index]\n",
    "    y_validation = split_y_train[validation_index]\n",
    "    X_train = split_X_train.iloc[training_index]\n",
    "    y_train = split_y_train[training_index]\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train_pca, X_validation_pca = data_preprocessing_pca(X_train, X_validation)\n",
    "\n",
    "    # Random Forest Classification\n",
    "    k = 4\n",
    "    skf = StratifiedKFold(k, random_state=None)\n",
    "\n",
    "    # Tuning the hyperparameters\n",
    "    grid_param = {'n_estimators': [10, 50, 100, 200, 400],'criterion': ['gini', 'entropy'],'bootstrap': [True, False]}\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid=grid_param, scoring='roc_auc', cv=skf, n_jobs=-1, verbose=2) \n",
    "    grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "    best_hyperparameters = grid_search.best_params_\n",
    "\n",
    "    # Best hyperparameters\n",
    "    n_estimators = best_hyperparameters.get('n_estimators')\n",
    "    criterion = best_hyperparameters.get('criterion')\n",
    "    bootstrap = best_hyperparameters.get('bootstrap')\n",
    "    print(f'Best number of trees: {(n_estimators)}')\n",
    "    print(f'Best split quality function: {(criterion)}')\n",
    "    print(f'Best bootstrap: {(bootstrap)}')\n",
    "    best_result = grid_search.best_score_\n",
    "\n",
    "    # Apply classifier with tuned hyperparameters\n",
    "    classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, bootstrap=bootstrap)\n",
    "    classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    classifier_predictions_test = classifier.predict(X_validation_pca)\n",
    "    accuracy = metrics.accuracy_score(y_validation, classifier_predictions_test)\n",
    "    print(f'Accuracy: {(accuracy)}')\n",
    "    print('-'*80)\n",
    "\n",
    "    # Learning curve\n",
    "    title = 'Learning curve for Random Forest Classifier'\n",
    "    ax = fig.add_subplot(2, 2, num + 1)\n",
    "    plot_learning_curve(classifier, title, X_train_pca, y_train, ax, ylim=(0.3, 1.01), cv=skf)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see in the results above, the accuracy is varying a lot between the folds. It can differ from approximately 0.55 to 0.80. It can be concluded that the random forest method with PCA preprocessing is not robust for this dataset.\n",
    "\n",
    "In the section below, the random forest classifier with Univariate Analysis as preprocessing is conducted."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: Random Forest\n",
    "# Preprocessing: Univariate Analysis\n",
    "# Dataset: train & validation\n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "# Loading of the data \n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of features: {len(data.columns)-1}')\n",
    "y_labels = data['label']\n",
    "del data['label']\n",
    "\n",
    "y = preprocessing.label_binarize(y_labels, ['T12', 'T34']) \n",
    "# 0 stands for T12 and 1 for T34\n",
    "y = [i[0] for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data in a train and test set\n",
    "split_X_train, split_X_test, split_y_train, split_y_test = train_test_split(data, y, stratify=y, test_size=0.2)\n",
    "\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "# Loop over the folds\n",
    "num = 0\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    X_train = split_X_train.iloc[training_index]\n",
    "    y_train = split_y_train[training_index]\n",
    "    X_validation = split_X_train.iloc[validation_index]\n",
    "    y_validation = split_y_train[validation_index]\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train_uni, X_validation_uni = data_preprocessing_uni_rf(X_train, y_train, X_validation)\n",
    "\n",
    "    # Random Forest Classification\n",
    "    k = 4\n",
    "    skf = StratifiedKFold(k, random_state=None) \n",
    "\n",
    "    # Tuning the hyperparameters\n",
    "    grid_param = {'n_estimators': [10, 50, 100, 200, 400],'criterion': ['gini', 'entropy'],'bootstrap': [True, False]}\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid=grid_param, scoring='roc_auc', cv=skf, n_jobs=-1, verbose=2)   \n",
    "    grid_search.fit(X_train_uni, y_train)\n",
    "\n",
    "    best_hyperparameters = grid_search.best_params_\n",
    "\n",
    "    # Best hyperparameters\n",
    "    n_estimators = best_hyperparameters.get('n_estimators')\n",
    "    criterion = best_hyperparameters.get('criterion')\n",
    "    bootstrap = best_hyperparameters.get('bootstrap')\n",
    "    print(f'Best number of trees: {(n_estimators)}')\n",
    "    print(f'Best split quality function: {(criterion)}')\n",
    "    print(f'Best bootstrap: {(bootstrap)}')\n",
    "    best_result = grid_search.best_score_  \n",
    "\n",
    "    # Apply classifier with tuned hyperparameters\n",
    "    classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, bootstrap=bootstrap)\n",
    "    classifier.fit(X_train_uni, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    classifier_predictions_test = classifier.predict(X_validation_uni)\n",
    "    accuracy = metrics.accuracy_score(y_validation, classifier_predictions_test)\n",
    "    print(f'Accuracy: {(accuracy)}')\n",
    "    print('-'*80)\n",
    "\n",
    "    # Learning curve\n",
    "    title = 'Learning curve for Random Forest Classifier'\n",
    "    ax = fig.add_subplot(2, 2, num + 1)\n",
    "    plot_learning_curve(classifier, title, X_train_uni, y_train, ax, ylim=(0.3, 1.01), cv=skf)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see in the results above, the accuracy is varying a lot. Random Forest with PCA as preprocessing results in a lower mean accuracy than Random Forest with Univariate Analysis as preprocessing. \n",
    "\n",
    "The overall conclusion is that the Random Forest classifier is not suitable for out dataset."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.b Support Vector Machine (SVM)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: SVM\n",
    "# Preprocessing: PCA\n",
    "# Dataset: train & validation\n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "# Loading of the data \n",
    "data = load_data()\n",
    "y_labels = data['label']\n",
    "del data['label']\n",
    "\n",
    "y = preprocessing.label_binarize(y_labels, ['T12', 'T34']) \n",
    "# 0 stands for T12 and 1 for T34\n",
    "y = [i[0] for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data in a train and test set\n",
    "split_X_train, split_X_test, split_y_train, split_y_test = train_test_split(data, y, stratify=y, test_size=0.2)\n",
    "\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "# Loop over the folds\n",
    "for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "    X_validation = split_X_train.iloc[validation_index]\n",
    "    y_validation = split_y_train[validation_index]\n",
    "    X_train = split_X_train.iloc[training_index]\n",
    "    y_train = split_y_train[training_index]\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train_pca, X_validation_pca = data_preprocessing_pca(X_train, X_validation)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    grid_param =[{'kernel': ('linear','rbf'), 'gamma': [1e-04, 1e-03, 1e-02, 0.1, 1, 10],'C': [1e-04, 1e-03, 1e-02, 0.1, 1, 10]}]\n",
    "    grid_search=GridSearchCV(SVC(),grid_param,n_jobs=-1,verbose=2)\n",
    "    grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "    best_hyperparameters = grid_search.best_params_\n",
    "\n",
    "    kernel=best_hyperparameters.get('kernel')\n",
    "    gamma=best_hyperparameters.get('gamma')\n",
    "    C=best_hyperparameters.get('C')\n",
    "\n",
    "    print(kernel)\n",
    "    print(gamma)\n",
    "    print(C)\n",
    "    best_result = grid_search.best_score_  \n",
    "\n",
    "    # Classification with tuned hyperparameters\n",
    "    clf_svm = svm.SVC(kernel=kernel, gamma=gamma, C=C)\n",
    "    clf_svm.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    clf_svm_predictions = clf_svm.predict(X_validation_pca)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_validation, clf_svm_predictions)\n",
    "    print(f'Accuracy: {(accuracy)}')\n",
    "    print('-'*80)\n",
    "\n",
    "    # Learning curve\n",
    "    title = 'Learning curve for SVM classifier'\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plot_learning_curve(clf_svm, title, X_train_pca, y_train, ax, ylim=(0.3, 1.01), cv=cv_4fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier: SVM\n",
    "# Preprocessing: Univariate Analysis\n",
    "# Dataset: train & validation\n",
    "# Use of GridsearchCV for parameter tuning\n",
    "\n",
    "# Loading of the data \n",
    "data = load_data()\n",
    "y_labels = data['label']\n",
    "del data['label']\n",
    "\n",
    "y = preprocessing.label_binarize(y_labels, ['T12', 'T34']) \n",
    "# 0 stands for T12 and 1 for T34\n",
    "y = [i[0] for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data in a train and test set\n",
    "split_X_train, split_X_test, split_y_train, split_y_test = train_test_split(data, y, stratify=y, test_size=0.2)\n",
    "\n",
    "cv_4fold = model_selection.StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "# Loop over the folds\n",
    "for training_index, validation_index in cv_4fold.split(split_X_train, split_y_train):\n",
    "    X_validation = split_X_train.iloc[validation_index]\n",
    "    y_validation = split_y_train[validation_index]\n",
    "    X_train = split_X_train.iloc[training_index]\n",
    "    y_train = split_y_train[training_index]\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train_uni, X_validation_uni = data_preprocessing_uni(X_train, y_train, X_validation)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    grid_param =[{'kernel': ('linear','rbf'), 'gamma': [1e-04, 1e-03, 1e-02, 0.1, 1, 10],'C': [1e-04, 1e-03, 1e-02, 0.1, 1, 10]}]\n",
    "    grid_search=GridSearchCV(SVC(),grid_param,n_jobs=-1,verbose=2)\n",
    "    grid_search.fit(X_train_uni, y_train)\n",
    "\n",
    "    best_hyperparameters = grid_search.best_params_\n",
    "\n",
    "    kernel=best_hyperparameters.get('kernel')\n",
    "    gamma=best_hyperparameters.get('gamma')\n",
    "    C=best_hyperparameters.get('C')\n",
    "\n",
    "    print(kernel)\n",
    "    print(gamma)\n",
    "    print(C)\n",
    "    best_result = grid_search.best_score_  \n",
    "\n",
    "    # Classification with tuned hyperparameters\n",
    "    clf_svm = svm.SVC(kernel=kernel, gamma=gamma, C=C)\n",
    "    clf_svm.fit(X_train_uni, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    clf_svm_predictions = clf_svm.predict(X_validation_uni)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_validation, clf_svm_predictions)\n",
    "    print(f'Accuracy: {(accuracy)}')\n",
    "    print('-'*80)\n",
    "\n",
    "    # Learning curve\n",
    "    title = 'Learning curve for SVM classifier'\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plot_learning_curve(clf_svm, title, X_train_uni, y_train, ax, ylim=(0.3, 1.01), cv=cv_4fold)"
   ]
  }
 ]
}